Robust Mole Detection Model Report: Overcoming Domain Shift

Following the "Smart Hans" failure of our initial model—which learned to detect medical camera artifacts rather than skin lesions—we implemented a rigorous "Robust Training" strategy to force the model to learn true dermatological features.

Methodology: Hard Mining & Data Destruction
We completely rebuilt the dataset and training pipeline to eliminate "easy" shortcuts.

Dataset Refinement: We removed generic objects (Caltech 101) and replaced them exclusively with "Hard Negatives": Oxford Pets (fur/eyes), DTD (abstract textures like 'blotchy' or 'dotted'), and Flowers102 (organic shapes/colors).

Anti-Artifact Pipeline: We applied "brutal" data augmentations during training, including aggressive cropping (to remove black corners/rulers), Gaussian blur and noise (to simulate phone camera quality), and random grayscale (to force structural analysis over color reliance).

The Outcome
The robust model achieved an accuracy of 99.39%. Unlike the suspicious 100% of the previous attempt, this score reflects genuine learning. The confusion matrix indicates excellent sensitivity, missing only 4 out of 1,352 moles, while correctly flagging difficult negative samples as "Not Moles."

Real-World Validation
Crucially, this model successfully classified a real-world smartphone photo of a mole, a test the previous model failed. This confirms that the model has successfully generalized beyond the domain of professional dermoscopy and is now analyzing the texture and structure of the lesion itself, effectively bridging the gap between medical imaging and consumer technology.